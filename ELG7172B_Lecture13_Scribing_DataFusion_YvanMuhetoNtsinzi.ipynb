{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ELG7172B_Lecture13_Scribing_DataFusion_YvanMuhetoNtsinzi.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/rajshekharM/Lecture-scribing/blob/master/ELG7172B_Lecture13_Scribing_DataFusion_YvanMuhetoNtsinzi.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "vNMCP5g3GWtt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Fusion for reducing measurement errors"
      ]
    },
    {
      "metadata": {
        "id": "T8sljGaYGWtt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Objectives\n",
        "Introducing data (sensor) fusion as a way to reduce measurement errors:\n",
        "<ul>\n",
        "<li>General overview of sensor fusion</li>\n",
        "<li>Review of two papers on sensor fusion for reducing measurement errors</li>\n",
        "</ul>\n"
      ]
    },
    {
      "metadata": {
        "id": "wc93TpElGWtu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Sensor fusion general overview"
      ]
    },
    {
      "metadata": {
        "id": "CjQ4nU7rGWtv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Definitions\n",
        "When it comes to defining sensore fusion, it is worth mentioning that there is some confusion in the terminology of fusion systems. In fact the terms information fusion, sensor fusion, data fusion, multisensor fusion are often used interchangeably depending on the author and/or the application. Let's present some definitions to clarify what the terms mean:\n",
        "<ul>\n",
        "<li>Information Fusion encompasses theory, techniques and tools conceived and employed for exploiting the synergy in the information acquired from multiple sources (sensor, databases, information gathered by human, etc.) such that the resulting decision or action is in some sense better (qualitatively or quantitatively, in terms of accuracy, robustness, etc.) than would be possible if any of these sources were used individually without such synergy exploitation[8].</li>\n",
        "<li>Data fusion is the process of integrating multiple data sources to produce more consistent, accurate, and useful information than that provided by any individual data source[1].</li>\n",
        "<li>Sensor Fusion is the combining of sensory data or data derived from sensory data such that the resulting information is in some sense better than would be possible when these sources were used individually[8].</li>\n",
        "<li>Sensor fusion is combining of sensory data or data derived from disparate sources such that the resulting information has less uncertainty than would be possible when these sources were used individually[2].</li>\n",
        "<li>The term multisensor fusion is defined similarly as sensor fusion although with an emphasis on the fact that the sensory data comes from multiple sensors.</li>\n",
        "</ul>\n",
        "These are not the only possible definitions for information fusion, data fusion and sensor fusion, in fact many authors have come up with many different definitions over the years, but the ones above should give a good general idea of what those terms mean and help clarify the above mentioned terminology confusion."
      ]
    },
    {
      "metadata": {
        "id": "hI2ixkG9GWtv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Introduction\n",
        "When introducing sensor fusion, a good analogy comes from nature. In fact, in nature, animals and human beings perform sensor fusion quite often. For example, when riding a bicycle humans have to combine information from their inner ear, visual system, their hands and feet to get a clear picture of their environment and their situation in it; e.g. when taking a turn they use their inner ear to maintain balance, they use their visual system to estimate the angle of the turn and stay on the path... in many situation information from more than just one sense is necessary. We perform data fusion without even knowing we are doing it.\n",
        "<br>As hinted by the definitions above sensor fusion is not just about combining information from multiple sensors, i.e. it should be distinguished from general \"multisensor integration\". The difference is illustrated in the figure below.\n",
        "<img src=\"Sensor_FusionVsIntegration.png\"> [copied from 8]\n"
      ]
    },
    {
      "metadata": {
        "id": "ZTBG4ZBMGWtw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Architectures and techniques for sensor fusion\n",
        "Due to the multiplicity of sensor fusion application there is no one size fit all sensor fusion architecture or technique. Instead, there is many architectures and techniques each with their advantages and disadvantages, some better suited than others for a given application. As sensor fusion is an active area of research new models and techniques are still coming up."
      ]
    },
    {
      "metadata": {
        "id": "koY6CDfTGWtx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Architectures\n",
        "Depending on the given application some architectures/models may be better suited than others. Let's introduce some of these architectures:\n",
        "<ul>\n",
        "<li>\n",
        "<u>JDL Fusion Architecture</u>: It was created at the US Joint Directors of Laboratories (JDL) in 1985 and is one of the, if not the, most popular model(s). The figure below illustrates the architecture: <img src=\"JDLArchitecture.png\"> [copied from 8]\n",
        "<br>The numbering of the different levels of the JDL do not represent a strict processing order, they can be parallelized. This model although quite popular has some drawbacks: it is very abstract, doesn't directly lead to actual real world implementation and its applications are not reusable.\n",
        "</li>\n",
        "<li>\n",
        "<u>Waterfall Fusion Process Model</u>: This model is very similar to the JDL model and as such suffers from the same drawbacks. It is illustrated by the figure below:<img src=\"WaterfallArchitecture.png\"> [copied from 8]\n",
        "<br>The stages of this model can be related the JDL levels as follows:\n",
        "    <ul>\n",
        "        <li>Sensing, signal processing :: Level 0</li>\n",
        "        <li>Feature extraction, pattern processing :: Level 1</li>\n",
        "        <li>Situation assessment :: Level 2</li>\n",
        "        <li>Decision making :: Level 3</li>\n",
        "    </ul>\n",
        "</li>\n",
        "<li>\n",
        "<u>Boyd Model</u>: This model also finds its root in military applications as it represents the classic decision-support mechanism in military information operations. This model is often used in sensor fusion and is illustrated by the figure below: <img src=\"BoydArchitecture.png\"> [copied from 8]\n",
        "<br>It can be related to the JDL model as follows:\n",
        "    <ul>\n",
        "        <li>Observe :: Sources</li>\n",
        "        <li>Orientate :: Levels 1, 2 and 3</li>\n",
        "        <li>Decide :: Level 4</li>\n",
        "        <li>Act :: No counterpart</li>\n",
        "    </ul>\n",
        "</li>\n",
        "<li>\n",
        "<u>Omnibus model</u>: This model defines a strict ordering and is cyclic in nature. It is meant to be used recursively at 2 levels of abstraction in the same application. It is illustrated by the figure below: <img src=\"OmnibusArchitecture.png\"> [copied from 8]\n",
        "<br>It has the drawback that it doesn't lead to a modular implementation.\n",
        "</li>\n",
        "</ul>"
      ]
    },
    {
      "metadata": {
        "id": "2bNtiA34GWtx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Techniques\n",
        "As mentioned above, sensor fusion is used in many disciplines and a such a wide variety of techniques/methods/approaches/frameworks have been developped over the years. All these techniques aim at solving different challenges encountered in sensor fusion which mostly arise from the nature of the application environment, the nature of the sensors and the nature of the data being fused. Here is a non exhaustive list of such challenges:\n",
        "<ul>\n",
        "    <li>Data imperfection</li>\n",
        "    <li>Outliers and spurious data</li>\n",
        "    <li>Conflicting data</li>\n",
        "    <li>Data modality</li>\n",
        "    <li>Data correlation</li>\n",
        "    <li>Data alignment/registration</li>\n",
        "    <li>Data association</li>\n",
        "    <li>Processing framework</li>\n",
        "    <li>Operational timing</li>\n",
        "    <li>Static versus dynamic phenomena</li>\n",
        "    <li>Data dimensionality</li>\n",
        "</ul>\n",
        "<br>[9] proposes a taxonomy based on data-related challenges typically addressed by sensor fusion techniques and uses this taxonomy to categorize sensor fusion techniques. The rationale behind this categorization is that despite the obvious differences in the sensor fusion applications they all fuse their input data and hence an input data related categorization makes sense. This figure below shows the taxonomy proposed in [9]: <img src=\"DataImperfectionTaxonomy.png\"> [copied from 9]"
      ]
    },
    {
      "metadata": {
        "id": "QwVLWq2ZGWty",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Sensor fusion techniques in case of imperfect data\n",
        "The first category of sensor fusion techniques has to do with data imperfection and its subcategories. The data imperfection challenge is the most fundamental problem in sensor fusion and as such catalyzes most of the research work in sensor fusion. The authors retained 7 approaches/theories that address the different types of input data imperfections. The figure below illustrates which data imperfection subcategory is tackled by which theory (DSET: Dempster-Shafer Evidence Theory): <img src=\"ImperfectDataTheories.png\"> [copied from 9] \n",
        "<br>And here is a comparative table of imperfect data sensor fusion approaches/frameworks\n",
        "<img src=\"ImperfectDataTheoriesComparison.png\"> [copied from 9] "
      ]
    },
    {
      "metadata": {
        "id": "FCPn_ja-GWty",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Sensor fusion techniques in case of correlated data\n",
        "Data correlation can lead to over or under confidence in the fusion results and is commonly caused by data incest. Data correlation is in most cases solved either by eliminating it or by acknowledging the presence of data correlation and taking it into account in the fusion process. Here is a summary table of the \"correlated data\" sensor fusion techniques/frameworks (CI: Covariance Intersection, KF: Kalman Filter): <img src=\"CorrelatedDataTechniquesSummary.png\"> [copied from 9] "
      ]
    },
    {
      "metadata": {
        "id": "0x3jmkfIGWtz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Sensor fusion techniques in case of inconsistent data\n",
        "This category considers inconsistency in the data due to the presence of spurious data or outliers, out of sequence data and conflicting data. Here is an overview of the \"inconsistent data\" sensor fusion techniques in tabular form (OOSM: Out Of Sequence Measurements, OOST: Out Of Sequence data for disordered Tracks): <img src=\"InconsistentDataTechniquesOverview.png\"> [copied from 9] "
      ]
    },
    {
      "metadata": {
        "id": "rSN9UyIrGWtz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Sensor fusion techniques in case of disparate data\n",
        "The disparateness of data is a result of the varied types of data sources e.g. sensors, humans... This disparateness of data makes the implementation of a fusion system able to handle it a quite complex task. The fusion of soft data (data generated by human), soft data and hard data is an area of research that is relatively new but is gaining traction."
      ]
    },
    {
      "metadata": {
        "id": "4VIkCPJgGWtz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The techniques mentionned above do not in any way represent an exhaustive list and as there is a lot of active research being done in sensor fusion, new techniques and paradigms are emerging."
      ]
    },
    {
      "metadata": {
        "id": "SYFdBrXPGWt0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Using data/sensor fusion for reducing measurement errors\n",
        "Data gathering in general and measurement in particular, more often than not, involve a certain degree of error and/or uncertainty. Therefore when one is combining multiple data sources each with their own errors and uncertainties he/she must be careful as to not amplify those errors and uncertainties but rather reduce them, and this is the purpose of data fusion. It is no surprise that this turns out to be extremely useful in many disciplines e.g. medecine, physics, chemistry, social sciences... and as such there are many data fusion techniques that have been developped over the years to tackle many different challenges in many different disciplines.\n",
        "<br>When we have access to multiple data sources with measurement errors, data fusion is a great way to extract a single/fused data set with reduced measurement errors. Depending on the type of data sources we have access to, we may choose different data fusion approaches. Let's look at a couple of common problems and the approaches proposed in [4] and [5] to tackle them. We will see that although the approaches used to tackle these two problems are different, they also have some commonalities as do the tackled problems."
      ]
    },
    {
      "metadata": {
        "id": "RtkCtDGvGWt0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Problem statement"
      ]
    },
    {
      "metadata": {
        "id": "R8a10JhbGWt1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Problem 1\n",
        "The problem considered in [4] is that of using a high quality dataset to improve the inferences on an error prone dataset.\n",
        "The case study considered in [4] is that of educational attainment misreporting in the American Community Survey (ACS) and the use of the more accurate yet different data in the National Survey of College Graduates (NSCG) to improve the inferences in the ACS; more specifically the 2010 ACS and 2010 NSCG are used as the error prone survey and the highly accurate survey respectively. The 1990 census long form (the predecessor of the ACS) and the 1993 NSCG which have a publicly available linked file are used as evidence of the kind of misreporting that one can expect in the ACS.\n",
        "<img src=\"Misreporting1990.png\"> [copied from 4]\n",
        "<br><br>Contrary to the 1990 ACS and 1993 NSCG that had a publicly available linked file that allowed to characterize the actual measurement error mechanism for educational attainment in the 1990 long form census, the 2010 ACS and NSCG have none. As a result of this lack of linked file, the 2010 NSCG is considered what is called \"gold standard\" dataset. As opposed to a validation dataset a gold standard dataset can be thought of as dataset that doesn't provide enough information to establish a direct linkage between the true values (NSCG in this case) and the error prone values (ACS in this case), instead only providing information about the distribution of the true values. The problem can be illustrated with the figure below:\n",
        "<img src=\"ProblemStatement1.png\"> [copied from 4]\n",
        "<br>In the figure above $D_E$ represent the ACS survey data, $D_G$ the NCSG survey data, Z the error prone measurements, Y the true values, X the variables in the samples, '?' what we don't observe, and 'v' what we observe."
      ]
    },
    {
      "metadata": {
        "id": "BTy0S8vOGWt2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Problem 2\n",
        "The case study used in [5] to demonstrate the efficacy of their data fusion method was stated as follows:\n",
        "<br>A MATLAB simulation in which the goal is to locate the position of a robot. The x position of the robot is obtained from two sensors and its y position is obtained from another two sensors. The uncertainty of all the sensors was modelled as white Gaussian noise with standard deviations 4.3cm and 6.8cm for the x position sensors and 4.5cm and 6.6cm for the y position sensors. Also, it was assumed that the robot moves at a constant speed of 7.8cm/sec in the x direction and 15.6cm/sec in the y direction. The robot was simulated to move for a duration of 20 seconds and the sampling time was 0.5 seconds. It is assumed that at each sampling time, measurements are obtained from all the sensors (imperfect measurements of the same true value)."
      ]
    },
    {
      "metadata": {
        "id": "FYt7tewjGWt3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The two problems stated above although different have some commonalities in the sense that they both include multiple source of data with different error and uncertainty specifications and they need to be fused to obtained a result that is more accurate and informative/useful than the separate data sources. They are however different in the sense that in one case they represent different values with one considered a true value (error prone values and gold standard) and in the other case they are all imperfect representation of the same true value."
      ]
    },
    {
      "metadata": {
        "id": "6b03Kct_GWt3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Proposed method and Results"
      ]
    },
    {
      "metadata": {
        "id": "8fTcfdvBGWt3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Method and results for problem 1\n",
        "[4] presents an approach that posits models for the reported values and the error process to improve the inference in ACS. The authors\n",
        "are trying to avoid the conditional independence assumption (CIA) between the error prone survey and the accurate one (given common variables X) as they consider it to be unrealistic.\n",
        "<br>The proposed method starts by making plausible assumptions about the error process, model them as such, combine these models with the distribution of the true data (from the NSCG in this case) and use mutliple imputation to predict the correct values (or corrections) of the error prone measurements from the ACS.\n",
        "<br>\n",
        "<br>In statistics, imputation is the process of replacing missing data with substituted values. Multiple imputation is a method for averaging outcomes across multiple imputed data sets to account for the problem of increased noise due to imputation [7].\n",
        "<br>\n",
        "<br>In [4] two main quantities are estimated from the ACS, i.e. the number of science and engineering degrees awarded to women and the average income across degrees. Only individuals less than 76 years old who reported at least a bachelor degree in the ACS are considered in the study and this is taken into account when estimating the parameters of the true data model.\n",
        "<br>The end goal is to estimate Pr(Y, Z | X), so that it can be used to create the multiple imputation mentioned above for the missing values of Y for the individuals in $D_E$\n",
        "<br>The full data likelihood for an individual i is modeled as:\n",
        "<br>$Pr(Y_i=k, Z_i=l | X_i) = Pr(Y_i=k | X_i) Pr(E_i = e|Y_i=k, X_i) Pr(Z_i=l | E_i=e, Y_i=k, X_i)$, where E is an indicator of reporting an error, i.e. $E_i=1$ when $Y_i \\neq Z_i$ and $E_i=0$ otherwise.\n",
        "<br>\n",
        "<br>$Pr(Y_i=k | X_i)$ is the true data model and [4] suggests using a fully staturated multinomial model. This model takes into account the informative sampling design of the NSCG.\n",
        "<br>\n",
        "<br>$Pr(E_i = e|Y_i=k, X_i)$ is the error model. In this study 2 set of model specifications were considered:\n",
        "<br>The first one uses flat priors for all parameters, meaning that no prior information is considered.\n",
        "<br>The second one uses informative priors derived from the 1993 linked file, mentioned above, on its parameters.\n",
        "<br>\n",
        "<br>$Pr(Z_i=l | E_i=1, Y_i=k, X_i)$ is the reporting model. $E_i=1$ as there is no error when $E_i=0$ (E is an indicator of reporting an error). In case of the second set of error model specification (the one with informative priors), the reporting model is similar to it but with different informative priors on its parameters.\n",
        "<br>\n",
        "<br>All the above models use M=50 for the multiple imputation.\n",
        "<br> \n",
        "<br>The tables below offer a preview of the type of error and reporting models considered in the study:\n",
        "<img src=\"FlatPriors.png\"> Summary of the first four measurement error model specifications for 2010 NSCG/ACS analysis. These models use flat prior distributions on all parameters. [copied from 4] \n",
        "<img src=\"InformativePriors.png\">  Summary of informative prior specification for 2010 NSCG/AXS analysis for males with bachelor's degrees. [copied from 4]\n",
        "<br>\n",
        "<br>The authors of [4] used a series of diagnostic tests to determine which model most likely reflects the true measurements error mechanism. In fact, there is no way to know for sure which model most closely reflects the true measurement error mechanism but the diagonstic tests allow to discount models that do not adequately described the observed data. Model 5 which which is one of the models using informative priors based on the 1993 linked file seems the most likely candidate. This finding is confirmed by the sensitivity tests and the independent check using the estimated average earnings in the 2010 Current Population Survey."
      ]
    },
    {
      "metadata": {
        "id": "tqK_4NQ-GWt4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Method and results for problem 2\n",
        "The method proposed in [5] combines a probabilistic fusion approach with Kalman filtering.\n",
        "<br>Probablistic fusion relies on Baye's theorem and probability distributions to express data uncertainty and perform fusion. Four different probabilistic fusion approaches were explored (for brevity only the first 2 are described and the formulas below assume the case where we have 2 sensors only not 4 as in the problem statement above):\n",
        "<ul>\n",
        "    <li>Simplified Bayesian Approach (SB): This approach naively uses the Baye's theorem to estimate the states X at time k given the measurements Z up to time k. Here is the posterior distribution: <br>$P(x_k|Z^k) = \\frac{p(z_k|x_k) p(x_k|Z^{k-1})}{p(Z^k|Z^{k-1})}$\n",
        "        where,\n",
        "        <br>$p(z_k|x_k)$ is the likelihood function and is based on the given sensor measurement model.\n",
        "        <br>$p(x_k|Z^{k-1})$ is the prior distribution and it incorporates the given transition model of the system.\n",
        "        <br>$p(X=x | Z = z_1,z_2) ∝ \\frac{1}{\\sqrt{2\\pi\\sigma^2_1}}\\exp(-\\frac{(x-z_1)^2}{2\\sigma^2_1}) \\frac{1}{\\sqrt{2\\pi\\sigma^2_2}}\\exp(-\\frac{(x-z_2)^2}{2\\sigma^2_2})$\n",
        "    </li>\n",
        "    <li>Modified BayesianApproach (MB): This approach builds on top of the SB approach to handle spurious data that can lead to inaccurate estimation by introducing a factor that increases/decreases the variance of the individual sensor distributions and as a result changes the variance of the fused posterior distribution. The factor is calculated as follows: \n",
        "        <br>$f = \\frac{M^2}{M^2 - (z_1 - z_2)^2}$ \n",
        "        <br>and the posterior \n",
        "        <br>$p(X=x | Z = z_1,z_2) ∝ \\frac{1}{\\sqrt{2\\pi\\sigma^2_1}}\\exp(-\\frac{(x-z_1)^2}{2\\sigma^2_1 f}) \\frac{1}{\\sqrt{2\\pi\\sigma^2_2}}\\exp(-\\frac{(x-z_2)^2}{2\\sigma^2_2 f})$.\n",
        "        <br>M is the maximum expected difference between the sensor 1 and 2 readings in this case.\n",
        "    </li>\n",
        "    <li>Geometric Redundant Fusion (GRF)</li>\n",
        "    <li>Heuristic Geometric Redundant Fusion (HGRF)</li>\n",
        "</ul>\n",
        "A comparative study of the 4 probabilistic fusion approaches above was conducted using metrics such as accuracy, computation time and fused variance and the MB approach came up on top.\n",
        "<br>The data fusion solution proposed in [5] is a combination of the Modified Bayesian Approach (MB) and Kalman filtering. The authors compared 3 techniques which mainly differ in the stage(s) at which the Kalman filtering is performed. These techniques are best described in the diagrams below.\n",
        "<img src=\"PreFiltering.png\"> Modified Bayesian with pre-filtering. [Copied from 5]\n",
        "<br>In this first technique, Kalman filtering is applied to the sensor data ($z_1, z_2,..., z_n$) and then the Modified Bayesian fusion is used on the Kalman filtered sensor outputs.\n",
        "<img src=\"PostFiltering.png\"> Modified Bayesian with post-filtering. [Copied from 5]\n",
        "<br>In this second technique, the Modified Bayesian fusion approach is applied on the sensor outputs ($z_1, z_2,..., z_n$) and then Kalman filtering is applied on the resulting fused data.\n",
        "<img src=\"PrePostFiltering.png\"> Modified Bayesian with pre and post-filtering. [Copied from 5]\n",
        "<br>In this third technique, Kalman filtering is applied twice, first on the sensor data ($z_1, z_2,..., z_n$), after which the Modified Bayesian fusion is applied on the filtered sensor data, then Kalman filtering is applied a second time, this time on the resulting fused data.\n",
        "<br>\n",
        "<br>Based on the chosen metrics:\n",
        "<ul>\n",
        "    <li>CPU time: Computation time</li> \n",
        "    <li>Residual Sum of Squares(RSS): $RSS = \\sum_{i=1}^n (position_{theoretical,i}- position_{estimated,i})^2$</li>\n",
        "    <li>variance(P): Variance of the estimated robot position</li>\n",
        "    <li>The Criterion function(CF): A function used to compute the quality of the estimate\n",
        "    <br> $CF = w_1 x \\frac{c_1}{c_{1 max}} + w_2 x \\frac{c_2}{c_{2 max}} + w_3 x \\frac{c_3}{c_{3 max}}$\n",
        "    <br>where $w_i$ represends the weight of the time, RSS and variance(set by the user $w_i \\epsilon [0,1]$) and $c_i$ are the values obtained for the time, RSS and variance, with $c_{i max}$ being the maximum values of each.\n",
        "    </li>\n",
        "</ul>\n",
        "The post filtering technique (MB-F) outperforms all others, next in line is the pre-post filtering techinque (F-MB-F), the main drawback of the F-MB-F techinque compared to MB-F is the high computation time. This is well illustrated in the tabe below:\n",
        "<img src=\"MB_filtering_results.png\"> Computational decision making chart for the fusion techniques using 2 sensors. [copied from 5]"
      ]
    },
    {
      "metadata": {
        "id": "AVHsMATSGWt4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The methods described above are certainly different as they are meant to address different problems however they do have some general commonalities. \n",
        "<br>Both methods start by modeling the data:\n",
        "<br>In the first problem the error, the true data and the reporting error are modeled using plausible assumptions and the 1993 linked file.\n",
        "<br>In the second problem the sensors are modeled (their errors and uncertainties).\n",
        "<br>Once the modeling is done, the models are combined with the actual data (or its distribution) along with some metrics/crieria (sensibility checks, diagnostic tests... in problem 1; Time, RSS, P, CF... in problem 2) to obtained the desired result(s)."
      ]
    },
    {
      "metadata": {
        "id": "jz3IfQsQGWt5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### References\n",
        "[1] https://en.wikipedia.org/wiki/Data_fusion\n",
        "<br>[2] https://en.wikipedia.org/wiki/Sensor_fusion\n",
        "<br>[3] https://github.com/datascopeanalytics/sensor_fusion\n",
        "<br>[4] Tracy Schifeling, Jerome P. Reiter and Maria DeYoreo, Data Fusion for Correcting Measurement Errors, Oct. 1, 2016. Available: https://arxiv.org/pdf/1610.00147.pdf\n",
        "<br>[5] Waleed A. Abdulhafiz and Alaa Khamis, Handling Data Uncertainty and Inconsistency Using Multisensor Data Fusion, Hindawi Publishing Corporation, Advances in Artificial Intelligence, Volume 2013, Artical ID 241260. Available: https://www.hindawi.com/journals/aai/2013/241260/\n",
        "<br>[6] http://www.uio.no/studier/emner/matnat/fys/FYS3240/v15/lectures/l13---data-fusion-and-estimation.pdf\n",
        "<br>[7] https://en.wikipedia.org/wiki/Imputation_(statistics)\n",
        "<br>[8] Wilfried Elmenreich, An Introduction to Sensor Fusion, Research Report 74/2001, Vienna University of Technology, Austria, November 19, 2002\n",
        "<br>[9] Bahador Khaleghi, Alaa Khamis, Fakhreddine O. Karray, Saiedeh N. Razavi, Multisensor data fusion: A review of the state-of-the-art, Information fusion 14 (2013) 28-44"
      ]
    }
  ]
}